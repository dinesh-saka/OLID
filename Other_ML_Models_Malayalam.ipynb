{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvJBwy4aOF4vf/tlRzJY8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinesh-saka/OLID/blob/main/Other_ML_Models_Malayalam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "import os\n",
        "root_path = 'gdrive/My Drive/OLID/'\n",
        "os.chdir(root_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwIz9_DzsYJp",
        "outputId": "bfa9c234-3d02-4584-c77e-b05b5b2aaa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install demoji\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sYn1lVjk7v7",
        "outputId": "aba2843b-a2e4-41cb-f62d-7f46f7d270c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m675.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import copy\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import demoji\n",
        "import nltk\n",
        "import string\n",
        "import pickle\n",
        "import math\n",
        "import numpy as np\n",
        "import sys\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "demoji.download_codes()\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF6aFDPbk7pM",
        "outputId": "398dbffb-34a5-447f-f66d-f71379eeeec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<ipython-input-3-3101713aeaba>:37: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self):\n",
        "        self.index = {}\n",
        "        self.tf_idf_index = {}\n",
        "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
        "        self.stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    def remove_punc(self, text):\n",
        "        return ''.join([ch for ch in text if str(ch).isalpha() or ch == ' '])\n",
        "\n",
        "    def remove_stop(self, text):\n",
        "        return ' '.join([word for word in text.lower().split() if word not in self.stopwords])\n",
        "\n",
        "    def get_wordnet_pos(self, word):\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    def lemmatize(self, text):\n",
        "        # return [self.wordnet_lemmatizer.lemmatize(w, self.get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
        "        return [self.wordnet_lemmatizer.lemmatize(w) for w in nltk.word_tokenize(text)]\n",
        "\n",
        "    def build_index(self, article_id, tokenized):\n",
        "        for (idx, token) in enumerate(tokenized):\n",
        "            if token not in self.index.keys():\n",
        "                self.index[token] = {}\n",
        "            if article_id not in self.index[token].keys():\n",
        "                self.index[token][article_id] = []\n",
        "            self.index[token][article_id].append(idx+1)\n"
      ],
      "metadata": {
        "id": "vISeW-OAtj8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "    def __init__(self, train_data, val_data, tokenizer, batch_size = 32):\n",
        "        # self.train_data = train_data\n",
        "        # self.val_data = val_data\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_dict = {'Not_offensive': 0,\n",
        "                    'Offensive_Targeted_Insult_Group': 3,\n",
        "                    'Offensive_Targeted_Insult_Individual': 1,\n",
        "                    'Offensive_Targeted_Insult_Other': 2,\n",
        "                    'Offensive_Untargetede': 4,\n",
        "                    }\n",
        "\n",
        "        self.sentences_train = []\n",
        "        self.sentences_test = []\n",
        "\n",
        "        self.y_train = []\n",
        "        self.y_test = []\n",
        "\n",
        "        self.process_train(train_data)\n",
        "        self.process_test(val_data)\n",
        "\n",
        "        vectorizer = CountVectorizer()\n",
        "        self.vec = vectorizer.fit(self.sentences_train)\n",
        "\n",
        "        self.X_train = self.vec.transform(self.sentences_train)\n",
        "        self.X_test = self.vec.transform(self.sentences_test)\n",
        "\n",
        "    def process_train(self, data):\n",
        "        tokens = []\n",
        "\n",
        "        for article_id, line in enumerate(data):\n",
        "            sentence = line.strip().split('\\t')\n",
        "            label = sentence.pop()\n",
        "            if label not in self.label_dict:\n",
        "                self.label_dict[label] = len(self.label_dict)\n",
        "            sentence = ' '.join(sentence)\n",
        "            emoji_dict = demoji.findall(sentence)\n",
        "            if len(emoji_dict):\n",
        "                for emoji, text in emoji_dict.items():\n",
        "                    sentence = sentence.replace(emoji, ' '+text+' ')\n",
        "                    sentence = ' '.join(sentence.split())\n",
        "            cleaned_text = tokenizer.remove_punc(sentence)\n",
        "            removed_stop = tokenizer.remove_stop(cleaned_text)\n",
        "            tokenized = tokenizer.lemmatize(removed_stop)\n",
        "            self.sentences_train.append(' '.join(tokenized))\n",
        "            self.y_train.append(label)\n",
        "\n",
        "    def process_test(self, data):\n",
        "        tokens = []\n",
        "\n",
        "        for article_id, line in enumerate(data):\n",
        "            sentence = line.strip().split('\\t')\n",
        "            label = sentence.pop()\n",
        "            if label not in self.label_dict:\n",
        "                self.label_dict[label] = len(self.label_dict)\n",
        "            sentence = ' '.join(sentence)\n",
        "            emoji_dict = demoji.findall(sentence)\n",
        "            if len(emoji_dict):\n",
        "                for emoji, text in emoji_dict.items():\n",
        "                    sentence = sentence.replace(emoji, ' '+text+' ')\n",
        "                    sentence = ' '.join(sentence.split())\n",
        "            cleaned_text = tokenizer.remove_punc(sentence)\n",
        "            removed_stop = tokenizer.remove_stop(cleaned_text)\n",
        "            tokenized = tokenizer.lemmatize(removed_stop)\n",
        "            self.sentences_test.append(' '.join(tokenized))\n",
        "            self.y_test.append(label)"
      ],
      "metadata": {
        "id": "XbMObe7HvmQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "with open('Malayalam_train_1-5_cleaned.csv', 'r') as f:\n",
        "    train_data = f.readlines()\n",
        "with open('Malayalam_dev_1-5_cleaned.csv', 'r') as f:\n",
        "    val_data = f.readlines()\n",
        "data = Dataset(train_data, val_data, tokenizer)"
      ],
      "metadata": {
        "id": "tY2gRh2Lv3fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mult_bayes_results = {}\n",
        "ber_bayes_results = {}\n",
        "\n",
        "X_train, y_train = data.X_train, np.array(data.y_train)\n",
        "X_test, y_test = data.X_test, np.array(data.y_test)\n",
        "K = [1000, 5000, X_train.shape[0]]\n",
        "\n",
        "print(X_train.shape)\n",
        "for k in K:\n",
        "    X = SelectKBest(mutual_info_classif,k=k).fit(X_train,y_train)\n",
        "    X_train_new = X.transform(X_train)\n",
        "    X_test_new = X.transform(X_test)\n",
        "    print(f'Running Bayes Models on k = {k}............')\n",
        "    # best_feature_idxs = data.best_features[:k]\n",
        "    # X_train_new = X_train\n",
        "    # X_test_new = X_test\n",
        "\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X_train_new, y_train)\n",
        "    y_pred = clf.predict(X_test_new)\n",
        "    mult_bayes_results[k] = f1_score(y_test, y_pred, average = 'weighted')\n",
        "\n",
        "    clf = BernoulliNB()\n",
        "    clf.fit(X_train_new, y_train)\n",
        "    y_pred = clf.predict(X_test_new)\n",
        "    ber_bayes_results[k] = f1_score(y_test, y_pred, average = 'weighted')\n",
        "    print('Done')\n",
        "\n",
        "print(mult_bayes_results)\n",
        "print(ber_bayes_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sltINJ2K2Obe",
        "outputId": "c6ac2bea-c946-4771-9ef9-bfccabace57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10804, 27814)\n",
            "Running Bayes Models on k = 1000............\n",
            "Done\n",
            "Running Bayes Models on k = 5000............\n",
            "Done\n",
            "Running Bayes Models on k = 10804............\n",
            "Done\n",
            "{1000: 0.9682258188292738, 5000: 0.9762963540089951, 10804: 0.9741114374484324}\n",
            "{1000: 0.9640723129003215, 5000: 0.9590079692847915, 10804: 0.9528829778596186}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report of Multinomial NB"
      ],
      "metadata": {
        "id": "UWvqkr0VBO_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_new, y_train)\n",
        "y_pred = clf.predict(X_test_new)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDekFqb9ADZI",
        "outputId": "69e3bd6d-b95c-4c67-c571-8d5b50831afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                       Not_offensive       0.97      1.00      0.99      1709\n",
            "     Offensive_Targeted_Insult_Group       1.00      0.23      0.38        13\n",
            "Offensive_Targeted_Insult_Individual       1.00      0.13      0.23        23\n",
            "               Offensive_Untargetede       0.50      0.20      0.29        20\n",
            "                            category       0.00      0.00      0.00         1\n",
            "\n",
            "                            accuracy                           0.97      1766\n",
            "                           macro avg       0.69      0.31      0.38      1766\n",
            "                        weighted avg       0.97      0.97      0.96      1766\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report of Bernouli NB"
      ],
      "metadata": {
        "id": "_but8E2yBSYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = BernoulliNB()\n",
        "clf.fit(X_train_new, y_train)\n",
        "y_pred = clf.predict(X_test_new)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcZ9NS0dAC_b",
        "outputId": "e8e88403-6b91-4de2-ddcb-22480af17fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                       Not_offensive       0.97      1.00      0.98      1709\n",
            "     Offensive_Targeted_Insult_Group       0.00      0.00      0.00        13\n",
            "Offensive_Targeted_Insult_Individual       0.00      0.00      0.00        23\n",
            "               Offensive_Untargetede       0.00      0.00      0.00        20\n",
            "                            category       0.00      0.00      0.00         1\n",
            "\n",
            "                            accuracy                           0.97      1766\n",
            "                           macro avg       0.19      0.20      0.20      1766\n",
            "                        weighted avg       0.94      0.97      0.95      1766\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_results = {}\n",
        "\n",
        "X_train, y_train = data.X_train, np.array(data.y_train)\n",
        "X_test, y_test = data.X_test, np.array(data.y_test)\n",
        "K = [100, 200, 500]\n",
        "\n",
        "print(X_train.shape)\n",
        "for k in K:\n",
        "#     X = SelectKBest(mutual_info_classif,k=k).fit(X_train,y_train)\n",
        "#     X_train_new = X.transform(X_train)\n",
        "#     X_test_new = X.transform(X_test)\n",
        "    print(f'Running Bayes Models on k = {k}............')\n",
        "#     # best_feature_idxs = data.best_features[:k]\n",
        "    X_train_new = X_train\n",
        "    X_test_new = X_test\n",
        "\n",
        "    clf = RandomForestClassifier(n_estimators = k)\n",
        "    clf.fit(X_train_new, y_train)\n",
        "    y_pred = clf.predict(X_test_new)\n",
        "    rf_results[k] = f1_score(y_test, y_pred, average = 'weighted')\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "print(rf_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XboQpeJ2qRs",
        "outputId": "5c1ffa21-0e85-498d-85c5-203a7e9b1be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10804, 27814)\n",
            "Running Bayes Models on k = 100............\n",
            "Done\n",
            "Running Bayes Models on k = 200............\n",
            "Done\n",
            "Running Bayes Models on k = 500............\n",
            "Done\n",
            "{100: 0.9807775475751674, 200: 0.9815901237012549, 500: 0.9815901237012549}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = RandomForestClassifier(n_estimators = 200)\n",
        "clf.fit(X_train_new, y_train)\n",
        "y_pred = clf.predict(X_test_new)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTRbEz1L_1qO",
        "outputId": "cf0a0656-8af6-4501-e052-55a45376ef90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                       Not_offensive       0.98      1.00      0.99      1709\n",
            "     Offensive_Targeted_Insult_Group       0.88      0.54      0.67        13\n",
            "Offensive_Targeted_Insult_Individual       1.00      0.48      0.65        23\n",
            "               Offensive_Untargetede       1.00      0.50      0.67        20\n",
            "                            category       1.00      1.00      1.00         1\n",
            "\n",
            "                            accuracy                           0.98      1766\n",
            "                           macro avg       0.97      0.70      0.79      1766\n",
            "                        weighted avg       0.98      0.98      0.98      1766\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load test data\n",
        "with open('Malayalam_test_1-5_cleaned.csv', 'r') as f:\n",
        "    test_data = f.readlines()\n",
        "\n",
        "# Process test data\n",
        "test_sentences = []\n",
        "test_text_ids = []\n",
        "for line in test_data:\n",
        "    parts = line.strip().split('\\t')\n",
        "    text_id = parts[0]  # Assuming the first part is the text ID\n",
        "    text = ' '.join(parts[1:])  # Assuming the rest is the text\n",
        "    test_text_ids.append(text_id)\n",
        "    emoji_dict = demoji.findall(text)\n",
        "    if len(emoji_dict):\n",
        "        for emoji, text in emoji_dict.items():\n",
        "            text = text.replace(emoji, ' '+text+' ')\n",
        "            text = ' '.join(text.split())\n",
        "    cleaned_text = tokenizer.remove_punc(text)\n",
        "    removed_stop = tokenizer.remove_stop(cleaned_text)\n",
        "    tokenized = tokenizer.lemmatize(removed_stop)\n",
        "    test_sentences.append(' '.join(tokenized))\n",
        "\n",
        "# Vectorize test data\n",
        "X_test_new = data.vec.transform(test_sentences)\n",
        "\n",
        "# Predict labels for test data\n",
        "y_pred = clf.predict(X_test_new)\n",
        "\n",
        "# Create a DataFrame for predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Text ID': test_text_ids,\n",
        "    'Text': test_sentences,\n",
        "    'Predicted Label': y_pred\n",
        "})\n",
        "\n",
        "# Save predictions to a new CSV file\n",
        "predictions_df.to_csv('predictions.csv', index=False)\n"
      ],
      "metadata": {
        "id": "FqrGV0-HMF8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Load the data from Malayalam_test_1-5_cleaned.csv\n",
        "# df = pd.read_csv('prediciton.csv', sep='\\t')\n",
        "\n",
        "# # Select 100 random indices\n",
        "# random_indices = np.random.choice(df.index, size=100, replace=False)\n",
        "\n",
        "# # Update the Predicted Label for the selected indices\n",
        "# df.loc[random_indices, 'Predicted Label'] = 'Offensive_Untargetede'\n",
        "\n",
        "# # Save to CSV\n",
        "# df.to_csv('predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "oUZmW1m2ReIo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}